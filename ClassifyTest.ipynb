{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import (\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    svm,\n",
    "    ensemble\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING PREPARATION - EXPLAINED IN THE OTHER NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_accidents = pd.read_csv(\n",
    "    'data/accidents.csv',\n",
    "    header=0\n",
    ")\n",
    "dataset_accidents = dataset_accidents.drop_duplicates()\n",
    "\n",
    "dataset_vehicles = pd.read_csv(\n",
    "    'data/vehicles.csv',\n",
    "    header=0\n",
    ")\n",
    "dataset_vehicles = dataset_vehicles.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of training set (explained in the other notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_accidents = dataset_accidents[\n",
    "    dataset_accidents['latitude'] >= 40\n",
    "]\n",
    "\n",
    "dropped_cols_begin = [\n",
    "    'location_easting_osgr', 'location_northing_osgr',\n",
    "    'lsoa_of_accident_location', 'latitude', 'longitude',\n",
    "    'local_authority_district', 'local_authority_highway',\n",
    "    'pedestrian_crossing-human_control',\n",
    "    'pedestrian_crossing-physical_facilities',\n",
    "    'carriageway_hazards', '1st_road_number', '2nd_road_number'\n",
    "]\n",
    "\n",
    "dataset_accidents.drop(dropped_cols_begin, axis=1, inplace=True)\n",
    "\n",
    "dataset_accidents['weekday'] = pd.to_datetime(\n",
    "    dataset_accidents['date']\n",
    ").dt.weekday_name\n",
    "dataset_accidents['weekend'] = (dataset_accidents['weekday'].isin(\n",
    "    ['Friday', 'Saturday', 'Sunday']\n",
    "))*1\n",
    "dataset_accidents['day_period'] = pd.to_datetime(\n",
    "    dataset_accidents['time']\n",
    ").dt.hour\n",
    "dataset_accidents['day_period'] = pd.cut(\n",
    "    dataset_accidents['day_period'], bins=[0, 7, 9, 13, 16, 20, 24], right=False\n",
    ")\n",
    "dataset_accidents['urban_area'] = 1*(\n",
    "    dataset_accidents['urban_or_rural_area'] == 'Urban'\n",
    ")\n",
    "\n",
    "dropped_cols_intermediate = ['date', 'time', 'weekday', 'urban_or_rural_area']\n",
    "dataset_accidents.drop(dropped_cols_intermediate, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGING BOTH DATASETS USING PRIMARY KEY TO HAVE INFORMATION FROM TWO SOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_accidents.merge(dataset_vehicles, on='accident_id')\n",
    "\n",
    "dataset = dataset.drop('accident_id', axis=1)\n",
    "\n",
    "dataset['Towing_and_Articulation'] = (~dataset['Towing_and_Articulation'].isin(\n",
    "    ['-1', 'No tow/articulation']\n",
    "))\n",
    "dataset['Carriageway_Left'] = (dataset['Vehicle_Leaving_Carriageway'] != 'Did not leave carriageway')*1\n",
    "\n",
    "dataset['Skidding_and_Overturning'] = dataset[\n",
    "    dataset['Skidding_and_Overturning'].isin(['-1', 'None'])\n",
    "]\n",
    "\n",
    "dropped_cols_end = [\n",
    "    'Sex_of_Driver', 'Vehicle_Reference',\n",
    "    'Vehicle_IMD_Decile', 'Driver_IMD_Decile',\n",
    "    'Was_Vehicle_Left_Hand_Drive?', 'Hit_Object_in_Carriageway',\n",
    "    'Driver_Home_Area_Type', 'Vehicle_Leaving_Carriageway',\n",
    "    'Junction_Location', 'Journey_Purpose_of_Driver'\n",
    "]\n",
    "\n",
    "dataset.drop(dropped_cols_end, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISSING DATA HANDLING\n",
    "### Missing values for categorical data are replaced with the mode of the variable (common point imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data = {\n",
    "    'Propulsion_Code': dataset['Propulsion_Code'].mode()[0],\n",
    "    'Vehicle_Type': dataset['Vehicle_Type'].mode()[0],\n",
    "    'Vehicle_Location-Restricted_Lane': dataset[\n",
    "        'Vehicle_Location-Restricted_Lane'\n",
    "    ].mode()[0],\n",
    "    'Skidding_and_Overturning': dataset['Skidding_and_Overturning'].mode()[0],\n",
    "    'Hit_Object_off_Carriageway': dataset['Hit_Object_off_Carriageway'].mode()[0],\n",
    "}\n",
    "\n",
    "for key, val in missing_data.items():\n",
    "    dataset.loc[dataset[key] == '-1', key] = val\n",
    "\n",
    "dataset = pd.get_dummies(dataset)\n",
    "dataset = pd.get_dummies(\n",
    "    dataset,\n",
    "    columns=['road_type', 'weather_conditions']\n",
    ")\n",
    "\n",
    "dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='-1')))]\n",
    "\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "x = dataset.drop('target', axis=1)\n",
    "y = dataset['target']\n",
    "\n",
    "classifier = ensemble.RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=20, class_weight='balanced'\n",
    ")\n",
    "\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        prediction\n",
      "id                \n",
      "372234           0\n",
      "332946           0\n",
      "596385           0\n",
      "493689           0\n",
      "420550           0\n",
      "387950           0\n",
      "597147           0\n",
      "425105           0\n",
      "449455           0\n",
      "561663           0\n",
      "390433           0\n",
      "362091           0\n",
      "387050           0\n",
      "427108           0\n",
      "556942           0\n",
      "380380           0\n",
      "481792           0\n",
      "593830           0\n",
      "500739           0\n",
      "442834           0\n",
      "524007           0\n",
      "421171           0\n",
      "453413           0\n",
      "373314           0\n",
      "322565           0\n",
      "453007           0\n",
      "458750           0\n",
      "378589           0\n",
      "329451           0\n",
      "388105           0\n",
      "...            ...\n",
      "594369           0\n",
      "314782           0\n",
      "397307           1\n",
      "565751           1\n",
      "573614           0\n",
      "419371           1\n",
      "492819           0\n",
      "366456           1\n",
      "451354           0\n",
      "581753           1\n",
      "438119           0\n",
      "338475           1\n",
      "491845           0\n",
      "394104           0\n",
      "314169           0\n",
      "538098           0\n",
      "404881           1\n",
      "328412           0\n",
      "356584           0\n",
      "431412           0\n",
      "545981           1\n",
      "492646           0\n",
      "461128           0\n",
      "409542           1\n",
      "495736           0\n",
      "491060           0\n",
      "426373           1\n",
      "382213           0\n",
      "463100           1\n",
      "401102           0\n",
      "\n",
      "[107438 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_test = pd.read_csv(\n",
    "    'data/test.csv',\n",
    "    header=0\n",
    ")\n",
    "\n",
    "dataset_test.drop(dropped_cols_begin, axis=1, inplace=True)\n",
    "\n",
    "dataset_test['weekday'] = pd.to_datetime(\n",
    "    dataset_test['date']\n",
    ").dt.weekday_name\n",
    "dataset_test['weekend'] = (dataset_test['weekday'].isin(\n",
    "    ['Friday', 'Saturday', 'Sunday']\n",
    "))*1\n",
    "dataset_test['day_period'] = pd.to_datetime(\n",
    "    dataset_test['time']\n",
    ").dt.hour\n",
    "dataset_test['day_period'] = pd.cut(\n",
    "    dataset_test['day_period'], bins=[0, 7, 9, 13, 16, 20, 24], right=False\n",
    ")\n",
    "dataset_test['urban_area'] = 1*(\n",
    "    dataset_test['urban_or_rural_area'] == 'Urban'\n",
    ")\n",
    "\n",
    "dataset_test.drop(dropped_cols_intermediate, axis=1, inplace=True)\n",
    "\n",
    "dataset_test = dataset_test.merge(dataset_vehicles, how=\"left\", on='accident_id')\n",
    "\n",
    "dataset_ids = dataset_test['accident_id']\n",
    "dataset_test = dataset_test.drop('accident_id', axis=1)\n",
    "\n",
    "dataset_test['Towing_and_Articulation'] = (~dataset_test['Towing_and_Articulation'].isin(\n",
    "    ['-1', 'No tow/articulation']\n",
    "))\n",
    "dataset_test['Carriageway_Left'] = (\n",
    "    dataset_test['Vehicle_Leaving_Carriageway'] != 'Did not leave carriageway'\n",
    ")*1\n",
    "\n",
    "dataset_test['Skidding_and_Overturning'] = dataset_test[\n",
    "    dataset_test['Skidding_and_Overturning'].isin(['-1', 'None'])\n",
    "]\n",
    "\n",
    "for key, val in missing_data.items():\n",
    "    dataset_test.loc[dataset_test[key] == '-1', key] = val\n",
    "\n",
    "dataset_test = pd.get_dummies(dataset_test)\n",
    "dataset_test = pd.get_dummies(\n",
    "    dataset_test,\n",
    "    columns=['road_type', 'weather_conditions']\n",
    ")\n",
    "\n",
    "dataset_test = dataset_test[\n",
    "    dataset_test.columns.drop(list(dataset_test.filter(regex='-1')))\n",
    "]\n",
    "\n",
    "missing_cols = set(x.columns) - set(dataset_test.columns)\n",
    "for col in missing_cols:\n",
    "    dataset_test[col] = 0\n",
    "\n",
    "dataset_test = dataset_test[x.columns]\n",
    "dataset_test[dataset_test.isna()] = 0\n",
    "predictions = classifier.predict(dataset_test)\n",
    "\n",
    "predictions_df = pd.DataFrame(\n",
    "    {'id': dataset_ids, 'prediction': predictions}\n",
    ")\n",
    "\n",
    "final_preds = predictions_df.groupby('id').mean()\n",
    "final_preds = final_preds.astype(int).reindex(dataset_ids.unique())\n",
    "print(final_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
